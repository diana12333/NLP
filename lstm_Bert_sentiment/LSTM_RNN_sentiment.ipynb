{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "9aOozaxFNG21",
    "outputId": "fa867ff7-6bc6-4b22-bd68-ed032ba4103a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  3 04:00:43 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P0    29W /  70W |  12989MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if a gpu is available\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czxD_qGxNMec"
   },
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "\n",
    "1. Use Pytorch to load the IMDb movie dataset and do preprocessing;\n",
    "2. Develop a Recurrent Neural Network (RNN) Classifier for the same dataset;\n",
    "3. Convert the RNN to a bidirectional Long-Short-Term-Memory (LSTM) model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kF3du_w9t8CE"
   },
   "source": [
    "## 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L67uXDrvNQhX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 12138\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Torchtext will let us to load the text and labels separately.\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vfizApcF8Yz1",
    "outputId": "452448f3-feec-47ec-b617-be429358f2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    " # follow the steps to authorize colab to get access to your google drive data\n",
    " from google.colab import drive\n",
    " drive.mount('/content/gdrive')\n",
    "#4/xAHVfTtagNdR6xFZQwe5kH4F_x4JZ2I-84q4gtffEeiJgWeaLicbcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdz9mWho82wc",
    "outputId": "1f4220a0-cf94-43c4-c266-c351297074fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'gdrive/My Drive/Colab Notebooks/nlp_hw2/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# make sure that you can see the ipynb files and IMDB.gz\n",
    "!ls  gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-5JsNlBNSX1"
   },
   "source": [
    "## Data loading\n",
    "Read more: https://pytorchnlp.readthedocs.io/en/latest/_modules/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0D2GMZSGNRlY",
    "outputId": "39ea95ac-4ef3-4d96-bec8-a1f9d98bbb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished!\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "import os\n",
    "\n",
    "# set up the path\n",
    "ROOT_DIR = \" gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/\"\n",
    "DATA_DIR = ROOT_DIR+'IMDB.gz'\n",
    "\n",
    "# load data, this may take a while\n",
    "all_data = datasets.IMDB(DATA_DIR,TEXT, LABEL)\n",
    "train_data, test_data = all_data.splits(TEXT, LABEL)\n",
    "\n",
    "print ('Loading finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GieDV9WdOatS",
    "outputId": "1da80370-4b59-460f-b291-03adc90a74b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KkloUWsTOzV3",
    "outputId": "41a1175f-2d12-4beb-f326-a6fd6f670091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# split into train and validation set\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUtcoLdTfYlQ"
   },
   "outputs": [],
   "source": [
    "# set vocab\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2GwtGZ7g9c1"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    ## fill here:\n",
    "    param_number = 0\n",
    "    param_number = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return param_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_aSJxDASDVe"
   },
   "source": [
    "##### Define iterator\n",
    "\n",
    "Define an iterator that batches examples of similar lengths together. \n",
    "There are other options. For more: https://torchtext.readthedocs.io/en/latest/data.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsdB0cfbO7g0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# If there is a GPU available, we will set to use it; otherwise we will use cpu.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW3gMNJdx8yV"
   },
   "source": [
    "## 2. Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8SCAP3ISI1v"
   },
   "source": [
    "The RNN model has the following structure:\n",
    "1. start by an embedding layer; shape:  (input_dim, embedding_dim)\n",
    "2. then we put the RNN layer; shape: (embedding_dim, hidden_dim)\n",
    "3. last, we add a liner layer; shape: (hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeouwCjNSISx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## TODO: define the RNN class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## TODO starts\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        ## TODO ends\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ## TODO starts\n",
    "        embeds = self.embedding(text)\n",
    "        output, hidden = self.rnn(embeds)\n",
    "        #print(output.shape)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        \n",
    "        result =  self.fc(hidden.squeeze(0))\n",
    "        ## TODO ends\n",
    "\n",
    "        return  result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5xkXjVAeUIc"
   },
   "source": [
    "## Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tudzd0gqenDc",
    "outputId": "e78509c1-bba3-4884-efed-5cb2ce05a6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n"
     ]
    }
   ],
   "source": [
    "# define some hyperparameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "print(INPUT_DIM)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "LEARNING_RATE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c_xjxleFeWte",
    "outputId": "9ac3d7be-af7b-4c7a-b959-583c0c7bf762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# apply our RNN model here\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tuj4LPQhflJy"
   },
   "outputs": [],
   "source": [
    "\n",
    "## setup device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLJnUrnPfpIC"
   },
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zAau631fxCG"
   },
   "outputs": [],
   "source": [
    "##  return the accuracy given the preditions (preds) and true values (y); acc should be a float number\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    acc = (rounded_preds == y).float() \n",
    "    acc = acc.sum() / len(acc)\n",
    "    #round predictions to the closest integer\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbYNoyaOgHZs"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmBvYxG0gEzY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "      #print(batch.text.shape[0])\n",
    "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "      predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "      loss = criterion(predictions, batch.label)\n",
    "      \n",
    "      acc = binary_accuracy(predictions, batch.label)\n",
    "      \n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJl5UQqFghRl"
   },
   "source": [
    "### Evaluation function\n",
    "\n",
    "This step is to copy and paste what you did in the training function into the evaluate function. This time, thereâ€™s no additional optimization after the predictions, loss, and accuracy are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5QUfm3cgnTR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fORHPtVHg1jh"
   },
   "source": [
    "### Start training\n",
    "It may take a few minutes in total. The validate accuracy is around 50-51%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "rhp0Ng6gg1_1",
    "outputId": "221440a5-9b3e-4f99-e78a-68a1b0d4f39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.697 | Train Acc: 49.89%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 50.17%\n",
      "\tTrain Loss: 0.700 | Train Acc: 49.54%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.57%\n",
      "\tTrain Loss: 0.697 | Train Acc: 50.18%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.17%\n",
      "\tTrain Loss: 0.697 | Train Acc: 49.72%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.22%\n",
      "\tTrain Loss: 0.697 | Train Acc: 50.37%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.12%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "# let's train 5 epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "      \n",
    "    # we keep track of the best model, and save it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prOfpKWAhHvy"
   },
   "source": [
    "### Restore the best model and evaluate\n",
    "\n",
    "The test accuracy is around 47%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPwMRve4hMGB",
    "outputId": "067f5982-ea25-4842-a2e7-aceab904a1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.690 | Test Acc: 53.38%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0H6wZ6MN2xuY"
   },
   "source": [
    "## 3. LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UtRFOSw21Du"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bidirectional):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,bidirectional = bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)        \n",
    "        ## CHANGE THESE DEFINITIONS\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.embedding(text)\n",
    "        output, (hidden,cell) = self.lstm(embeds)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim = 1)\n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jue9iw1K24sz",
    "outputId": "96a2ccaf-f920-40a0-facb-7b825d9d0145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,233,897 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BIDIRECTIONAL)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "## setup device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01yngM1yDbuu"
   },
   "source": [
    "It may take a few minutes in total. The validate accuracy is around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2WpHP5eH3JoX",
    "outputId": "9968ab38-2a9f-485b-d344-ab6d4f97a194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.685 | Train Acc: 55.86%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 58.40%\n",
      "\tTrain Loss: 0.624 | Train Acc: 65.51%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 66.50%\n",
      "\tTrain Loss: 0.544 | Train Acc: 72.76%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 72.89%\n",
      "\tTrain Loss: 0.449 | Train Acc: 79.74%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 78.55%\n",
      "\tTrain Loss: 0.391 | Train Acc: 82.95%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 80.55%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_valid_loss = float('inf')\n",
    "# let's train 5 epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "      \n",
    "    # we keep track of the best model, and save it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_LSTM.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tvLbKSVG4UPs",
    "outputId": "94141a03-6b3d-4384-ad1c-4eab30dc08f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.489 | Test Acc: 78.56%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_LSTM.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvxJYUfW4Yxk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGp1HgQu4rrH"
   },
   "source": [
    "\n",
    "1.   LSTM can solve the problem of gradient vanishing problem perfectly by an extra recurrent state called a cell.\n",
    "2.   LSTM takes less resource and time to train on.\n",
    "3. LSTM has a higher accuracy because it's more controllable compared to RNN.\n",
    "\n",
    "Compare to the model complexity:\n",
    "1. LSTM has more parameters, and thus would take more training time (which we can calculated with `import time`)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of PartB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
