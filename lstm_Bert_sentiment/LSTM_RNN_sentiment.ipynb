{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "9aOozaxFNG21",
    "outputId": "fa867ff7-6bc6-4b22-bd68-ed032ba4103a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  3 04:00:43 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P0    29W /  70W |  12989MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check if a gpu is available\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czxD_qGxNMec"
   },
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "\n",
    "1. Use Pytorch to load the IMDb movie dataset and do preprocessing;\n",
    "2. Develop a Recurrent Neural Network (RNN) Classifier for the same dataset;\n",
    "3. Convert the RNN to a bidirectional Long-Short-Term-Memory (LSTM) model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kF3du_w9t8CE"
   },
   "source": [
    "## 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L67uXDrvNQhX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 12138\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Torchtext will let us to load the text and labels separately.\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vfizApcF8Yz1",
    "outputId": "452448f3-feec-47ec-b617-be429358f2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    " # follow the steps to authorize colab to get access to your google drive data\n",
    " from google.colab import drive\n",
    " drive.mount('/content/gdrive')\n",
    "#4/xAHVfTtagNdR6xFZQwe5kH4F_x4JZ2I-84q4gtffEeiJgWeaLicbcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdz9mWho82wc",
    "outputId": "1f4220a0-cf94-43c4-c266-c351297074fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'gdrive/My Drive/Colab Notebooks/nlp_hw2/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# make sure that you can see the ipynb files and IMDB.gz\n",
    "!ls  gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V-5JsNlBNSX1"
   },
   "source": [
    "## Data loading\n",
    "Read more: https://pytorchnlp.readthedocs.io/en/latest/_modules/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0D2GMZSGNRlY",
    "outputId": "39ea95ac-4ef3-4d96-bec8-a1f9d98bbb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished!\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets\n",
    "import os\n",
    "\n",
    "# set up the path\n",
    "ROOT_DIR = \" gdrive/My\\ Drive/Colab\\ Notebooks/nlp_hw2/\"\n",
    "DATA_DIR = ROOT_DIR+'IMDB.gz'\n",
    "\n",
    "# load data, this may take a while\n",
    "all_data = datasets.IMDB(DATA_DIR,TEXT, LABEL)\n",
    "train_data, test_data = all_data.splits(TEXT, LABEL)\n",
    "\n",
    "print ('Loading finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GieDV9WdOatS",
    "outputId": "1da80370-4b59-460f-b291-03adc90a74b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KkloUWsTOzV3",
    "outputId": "41a1175f-2d12-4beb-f326-a6fd6f670091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# split into train and validation set\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUtcoLdTfYlQ"
   },
   "outputs": [],
   "source": [
    "# set vocab\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2GwtGZ7g9c1"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    ## fill here:\n",
    "    param_number = 0\n",
    "    param_number = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return param_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_aSJxDASDVe"
   },
   "source": [
    "##### Define iterator\n",
    "\n",
    "Define an iterator that batches examples of similar lengths together. \n",
    "There are other options. For more: https://torchtext.readthedocs.io/en/latest/data.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsdB0cfbO7g0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# If there is a GPU available, we will set to use it; otherwise we will use cpu.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW3gMNJdx8yV"
   },
   "source": [
    "## 2. Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8SCAP3ISI1v"
   },
   "source": [
    "The RNN model has the following structure:\n",
    "1. start by an embedding layer; shape:  (input_dim, embedding_dim)\n",
    "2. then we put the RNN layer; shape: (embedding_dim, hidden_dim)\n",
    "3. last, we add a liner layer; shape: (hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeouwCjNSISx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "## TODO: define the RNN class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ## TODO starts\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        ## TODO ends\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        ## TODO starts\n",
    "        embeds = self.embedding(text)\n",
    "        output, hidden = self.rnn(embeds)\n",
    "        #print(output.shape)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        \n",
    "        result =  self.fc(hidden.squeeze(0))\n",
    "        ## TODO ends\n",
    "\n",
    "        return  result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5xkXjVAeUIc"
   },
   "source": [
    "## Model Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tudzd0gqenDc",
    "outputId": "e78509c1-bba3-4884-efed-5cb2ce05a6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25002\n"
     ]
    }
   ],
   "source": [
    "# define some hyperparameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "print(INPUT_DIM)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "LEARNING_RATE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c_xjxleFeWte",
    "outputId": "9ac3d7be-af7b-4c7a-b959-583c0c7bf762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# apply our RNN model here\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()#nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tuj4LPQhflJy"
   },
   "outputs": [],
   "source": [
    "\n",
    "## setup device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLJnUrnPfpIC"
   },
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zAau631fxCG"
   },
   "outputs": [],
   "source": [
    "##  return the accuracy given the preditions (preds) and true values (y); acc should be a float number\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    acc = (rounded_preds == y).float() \n",
    "    acc = acc.sum() / len(acc)\n",
    "    #round predictions to the closest integer\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbYNoyaOgHZs"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HmBvYxG0gEzY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "      #print(batch.text.shape[0])\n",
    "      optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "      predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "      loss = criterion(predictions, batch.label)\n",
    "      \n",
    "      acc = binary_accuracy(predictions, batch.label)\n",
    "      \n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJl5UQqFghRl"
   },
   "source": [
    "### Evaluation function\n",
    "\n",
    "This step is to copy and paste what you did in the training function into the evaluate function. This time, there’s no additional optimization after the predictions, loss, and accuracy are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5QUfm3cgnTR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fORHPtVHg1jh"
   },
   "source": [
    "### Start training\n",
    "It may take a few minutes in total. The validate accuracy is around 50-51%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "rhp0Ng6gg1_1",
    "outputId": "221440a5-9b3e-4f99-e78a-68a1b0d4f39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.697 | Train Acc: 49.89%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 50.17%\n",
      "\tTrain Loss: 0.700 | Train Acc: 49.54%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 50.57%\n",
      "\tTrain Loss: 0.697 | Train Acc: 50.18%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 49.17%\n",
      "\tTrain Loss: 0.697 | Train Acc: 49.72%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 48.22%\n",
      "\tTrain Loss: 0.697 | Train Acc: 50.37%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.12%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "# let's train 5 epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "      \n",
    "    # we keep track of the best model, and save it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "prOfpKWAhHvy"
   },
   "source": [
    "### Restore the best model and evaluate\n",
    "\n",
    "The test accuracy is around 47%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPwMRve4hMGB",
    "outputId": "067f5982-ea25-4842-a2e7-aceab904a1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.690 | Test Acc: 53.38%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0H6wZ6MN2xuY"
   },
   "source": [
    "## 3. LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UtRFOSw21Du"
   },
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bidirectional):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,bidirectional = bidirectional)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)        \n",
    "        ## CHANGE THESE DEFINITIONS\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.embedding(text)\n",
    "        output, (hidden,cell) = self.lstm(embeds)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim = 1)\n",
    "        #print(output.shape)\n",
    "        #print(hidden.shape)\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jue9iw1K24sz",
    "outputId": "96a2ccaf-f920-40a0-facb-7b825d9d0145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,233,897 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BIDIRECTIONAL)\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "## setup device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01yngM1yDbuu"
   },
   "source": [
    "It may take a few minutes in total. The validate accuracy is around 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2WpHP5eH3JoX",
    "outputId": "9968ab38-2a9f-485b-d344-ab6d4f97a194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.685 | Train Acc: 55.86%\n",
      "\t Val. Loss: 0.672 |  Val. Acc: 58.40%\n",
      "\tTrain Loss: 0.624 | Train Acc: 65.51%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 66.50%\n",
      "\tTrain Loss: 0.544 | Train Acc: 72.76%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 72.89%\n",
      "\tTrain Loss: 0.449 | Train Acc: 79.74%\n",
      "\t Val. Loss: 0.472 |  Val. Acc: 78.55%\n",
      "\tTrain Loss: 0.391 | Train Acc: 82.95%\n",
      "\t Val. Loss: 0.452 |  Val. Acc: 80.55%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_valid_loss = float('inf')\n",
    "# let's train 5 epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "      \n",
    "    # we keep track of the best model, and save it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_LSTM.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tvLbKSVG4UPs",
    "outputId": "94141a03-6b3d-4384-ad1c-4eab30dc08f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.489 | Test Acc: 78.56%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_LSTM.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvxJYUfW4Yxk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGp1HgQu4rrH"
   },
   "source": [
    "\n",
    "1.   LSTM can solve the problem of gradient vanishing problem perfectly by an extra recurrent state called a cell.\n",
    "2.   LSTM takes less resource and time to train on.\n",
    "3. LSTM has a higher accuracy because it's more controllable compared to RNN.\n",
    "\n",
    "Compare to the model complexity:\n",
    "1. LSTM has more parameters, and thus would take more training time (which we can calculated with `import time`)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of PartB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
